{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znrE3um64oWH"
      },
      "source": [
        "# Service Intelligence Practice\n",
        "- TA : Jongkyung Shin\n",
        "-  contact : shinjk1156@unist.ac.kr\n",
        "-  if you have some questions, feel free to email me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD6_6e-G4oWJ"
      },
      "source": [
        "## Loading dataset\n",
        "- Datasets: Hotel review data from hotel website in Singapore extracted using web crawling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYkKqZuP4oWJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_star2 = pd.read_excel(\"HotelRev_less4STAR.xlsx\", sheet_name= '2class')\n",
        "df_star3 = pd.read_excel(\"HotelRev_less4STAR.xlsx\", sheet_name= '3class')\n",
        "df_star4 = pd.read_excel(\"HotelRev_less4STAR.xlsx\", sheet_name= '4class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDySI5xk4oWK"
      },
      "outputs": [],
      "source": [
        "def makeClearSent(sent):\n",
        "    sent = str(sent)\n",
        "    sent = sent.replace(\"\\n\", \"\")\n",
        "    sent = sent.replace(\"\\r\", \"\")\n",
        "    sent = sent.replace(\"  \", \" \")\n",
        "    return sent\n",
        "\n",
        "\n",
        "def makeReviewList(df):\n",
        "    review_list = []\n",
        "    for i in range(len(df)):\n",
        "        review_list.append(makeClearSent(df.loc[i, \"title\"]) + \". \" + makeClearSent(df.loc[i, \"body\"]))\n",
        "\n",
        "    '''\n",
        "    --- same as ---- \n",
        "    review_list = [makeClearSent(df.loc[i, \"title\"]) + \". \" + makeClearSent(df.loc[i, \"body\"]) for i in range(len(df))]\n",
        "    '''\n",
        "    return review_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbp3busE4oWL"
      },
      "source": [
        "-  Extract only reviews and construct datasets (+ simple clearing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcCL4Ciu4oWL"
      },
      "outputs": [],
      "source": [
        "star2_review_list = makeReviewList(df_star2)\n",
        "star3_review_list = makeReviewList(df_star3)\n",
        "star4_review_list = makeReviewList(df_star4)\n",
        "\n",
        "star2_rating_list = (df_star2.loc[:,\"rating\"]//10).to_list()\n",
        "star3_rating_list = (df_star3.loc[:,\"rating\"]//10).to_list()\n",
        "star4_rating_list = (df_star4.loc[:,\"rating\"]//10).to_list()\n",
        "\n",
        "\n",
        "StarRatingList = star2_rating_list + star3_rating_list + star4_rating_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-71C2oA74oWL"
      },
      "source": [
        "## 1) Online Review Preprocessing\n",
        "\n",
        "* Using package : nltk \n",
        "\n",
        "  \n",
        "* Required steps\n",
        "  - Tokenization\n",
        "  - Lowercasing\n",
        "  - Lemmatization\n",
        "  - Stopwords removal\n",
        "  - pos tagging (you have to exact the only noun words)\n",
        "  - stemming etc.. (you can skip the stemming and other preprocessing methods)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qi6WF1ThBJj",
        "outputId": "b8a3c24c-4de1-44ca-b31d-7003c331ebff"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lTjymVL4oWM",
        "outputId": "78540c05-07f4-4c89-f949-2b5a12198c6c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWqeYM7I4oWM",
        "outputId": "51215983-9d2c-46dd-a061-7574fb82aa36"
      },
      "outputs": [],
      "source": [
        "ExampleReview = star2_review_list[0]\n",
        "print(ExampleReview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-msKNSpu4oWN"
      },
      "source": [
        "### 1-1) Tokenization\n",
        "- Use 'sent_tokenize' and 'word_tokenize' functions in nltk package\n",
        "- It is required two steps; sentence tokenization and word tokenization.\n",
        "- example : \"Expect Pure Misery.\" -> Tokenization -> ['Expect', 'Pure', 'Misery', '.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNp6X3F34oWN",
        "outputId": "f2d4f918-dfe5-412f-e788-a68e3870fa1e"
      },
      "outputs": [],
      "source": [
        "SentenceTokenList = sent_tokenize(ExampleReview)\n",
        "print(\"The number of sentense : {}\".format(len(SentenceTokenList)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTKYAf844oWN",
        "outputId": "f7bbfe52-d8c6-46b3-d102-84cc77fcec30"
      },
      "outputs": [],
      "source": [
        "ExampleSentence = SentenceTokenList[10]\n",
        "\n",
        "ExampleWordTokenList = word_tokenize(ExampleSentence)\n",
        "\n",
        "print(\"The number of words in a sentence : {}\\n\".format(len(ExampleWordTokenList)))\n",
        "print(\"Original sentence : \",ExampleSentence + \"\\n\")\n",
        "print(\"Tokenized sentence : \", ExampleWordTokenList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eiyicEj4oWO"
      },
      "source": [
        "### 1-2) Lowercasing\n",
        "- Lowercasing is required to handle words mixed with uppercase letters as semantically equal\n",
        "- example : Room -> room, Staff -> staff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SufCHBmC4oWO",
        "outputId": "da5a9278-e217-413e-fd79-bbf90c9edb2f"
      },
      "outputs": [],
      "source": [
        "print(\"Original sentence : \",ExampleSentence + \"\\n\")\n",
        "\n",
        "LowerCasedSentence = ExampleSentence.lower() # for string\n",
        "print(\"Lowercased Sentence : \", LowerCasedSentence)\n",
        "\n",
        "print(\"\\nOriginal word token list : \", ExampleWordTokenList)\n",
        "\n",
        "LowerCasedTokenList = []\n",
        "for token in ExampleWordTokenList:\n",
        "    LowerCapToken = token.lower() # for string\n",
        "    LowerCasedTokenList.append(LowerCapToken)\n",
        "    \n",
        "print(\"\\nLowercased word token list : \", LowerCasedTokenList)\n",
        "\n",
        "ExampleWordTokenList = LowerCasedTokenList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX7p2tr74oWO"
      },
      "source": [
        "### 1-3) Lemmatization\n",
        "- Lemmatization is required to handle inflectional forms as semantically equal\n",
        "- Example : rooms -> room, staffs -> staff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "negdCT2W4oWO",
        "outputId": "478c4468-baf6-4e43-ac8c-0c94486b8847"
      },
      "outputs": [],
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "LemmatizationTokenList = []\n",
        "for token in ExampleWordTokenList:\n",
        "    LemmatizedToken = lemma.lemmatize(token)\n",
        "    LemmatizationTokenList.append(LemmatizedToken)\n",
        "\n",
        "print(\"\\nOriginal word token list : \", ExampleWordTokenList)\n",
        "\n",
        "print(\"\\nLemmatized word token list : \", LemmatizationTokenList)\n",
        "\n",
        "ExampleWordTokenList = LemmatizationTokenList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNiTF84E4oWO"
      },
      "source": [
        "### 1-4) Stopwords removal\n",
        "- Stopword : Words that appear frequently in a sentence but are not related to the meaning of the sentence\n",
        "- Use 'stopword' function in nltk package to get the list of stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A34jRVqK4oWO",
        "outputId": "4b4882ea-b5b1-4c53-afff-8773913df99f"
      },
      "outputs": [],
      "source": [
        "StopWordList = stopwords.words('english')\n",
        "print(StopWordList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ig7hesd4oWP",
        "outputId": "42ee3de2-5c8f-4f9c-c242-f02297ad68a9"
      },
      "outputs": [],
      "source": [
        "StopWordRemovalTokenList = []\n",
        "for token in ExampleWordTokenList:\n",
        "    if token not in StopWordList:\n",
        "        StopWordRemovalTokenList.append(token)\n",
        "\n",
        "print(\"\\nOriginal word token list : \", ExampleWordTokenList)\n",
        "        \n",
        "print(\"\\nStopword Removed token list : \", StopWordRemovalTokenList)\n",
        "\n",
        "\n",
        "ExampleWordTokenList = StopWordRemovalTokenList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R3VMQFx4oWP"
      },
      "source": [
        "### 1-5) Part-Of-Speech Tagging (Pos tagging)\n",
        "\n",
        "- POS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech (e.g., nouns, verbs, adjectives, and adverbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbsEtuic4oWP",
        "outputId": "e10700d1-27ee-400b-d1c4-ce2b46ae81eb"
      },
      "outputs": [],
      "source": [
        "PosTaggedTokenList = pos_tag(ExampleWordTokenList)\n",
        "\n",
        "print(\"\\nOriginal word token list : \", ExampleWordTokenList)\n",
        "        \n",
        "print(\"\\nPos tagged token list : \", PosTaggedTokenList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyxJl1mK4oWP"
      },
      "source": [
        "### 1-6) Noun Extraction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGfw4xbN4oWP",
        "outputId": "38b00fc2-9b02-4896-977d-73551ebe8c5a"
      },
      "outputs": [],
      "source": [
        "NounTokenList = []\n",
        "for token, pos in PosTaggedTokenList:\n",
        "    if pos[0] == \"N\":\n",
        "        NounTokenList.append(token)\n",
        "        \n",
        "print(\"\\nOriginal word token list : \", ExampleWordTokenList)\n",
        "        \n",
        "print(\"\\nNoun token list : \", NounTokenList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYkJf5wB4oWP"
      },
      "source": [
        "### Your work (1)\n",
        "\n",
        "- I showed the example of each preprocessing result for one sample sentence.\n",
        "- You need to preprocess all the review data (star2, star3, and star4).\n",
        "- There are two datasets that you have to create; preprocessed token dataset (list) & Noun token dataset (list)\n",
        "- Output Variables name\n",
        "  - TokenDataset (preprocessed token dataset, review\\*sentence\\*word matrix)\n",
        "  - NounTokenDataset (Noun token dataset, review*noun matrix )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On_9rFal4oWQ",
        "outputId": "8e70ddc8-ae0c-4b0a-d009-b0e8586eef90"
      },
      "outputs": [],
      "source": [
        "ReviewList = star2_review_list + star3_review_list + star4_review_list\n",
        "NounTokenDataset = []\n",
        "TokenDataset = []\n",
        "################ Write Your code #####################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPaBhTCG4oWQ"
      },
      "source": [
        "#### Output datasets shape checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr0dr53d4oWQ",
        "outputId": "c16fb538-9325-44f6-d653-42b27c6f505c"
      },
      "outputs": [],
      "source": [
        "print(\"\\nThe number of reviews in TokenDataset : \", len(TokenDataset)) # Answer : about 33,132 \n",
        "print(\"\\nThe number of sentences in first review : \", len(TokenDataset[0])) # Answer : about 38 (It's okay if the value is not the same)\n",
        "print(\"\\nThe number of tokens in first sentence of first review : \", len(TokenDataset[0][0])) # Answer : about 4 (It's okay if the value is not the same)\n",
        "\n",
        "print(\"\\nThe number of reviews in NounTokenDataset : \", len(NounTokenDataset)) # Answer : about 33,132 \n",
        "print(\"\\nThe number of noun tokens in first review : \", len(NounTokenDataset[0])) # Answer : about 96 (It's okay if the value is not the same)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JmmAJW-4oWQ"
      },
      "source": [
        "## 2) LDA topic modeling (service feature engineering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HR7N3qx4oWQ",
        "outputId": "aafc9a64-85d8-4926-fe42-484787f0fdc4"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBiFv__P4oWQ"
      },
      "outputs": [],
      "source": [
        "#Create dictionary\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "id2word= corpora.Dictionary(NounTokenDataset)\n",
        "\n",
        "# if you want, use filter_extremes()\n",
        "id2word.filter_extremes(no_below=10, no_above=0.3)\n",
        "\n",
        "#create count matrix\n",
        "corpus = [id2word.doc2bow(rev) for rev in NounTokenDataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdzQJDSt4oWQ",
        "outputId": "633fde66-176a-49b7-c5b7-351eaf3b0644"
      },
      "outputs": [],
      "source": [
        "# Peform LDA model\n",
        "model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                        id2word=id2word,\n",
        "                                        num_topics=10,\n",
        "                                        iterations=3000,\n",
        "                                        alpha=0.1,\n",
        "                                        eta=0.01)\n",
        "model.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "HLclPnPV4oWQ",
        "outputId": "28960efa-c791-4b9f-a3aa-da37bade5fe3"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis \n",
        "#Visualize LDA model\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "vis = gensimvis.prepare(model, corpus, id2word)\n",
        "\n",
        "vis\n",
        "# pyLDAvis.save_html(vis, 'LDA visualization.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxXhIzZbMv6a"
      },
      "outputs": [],
      "source": [
        "RawTopicTokenList = model.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BbZKH2xNCUm"
      },
      "outputs": [],
      "source": [
        "def parse_topic_token(RawTopicTokenList):\n",
        "  TopicDic = []\n",
        "  for idx, eqs in RawTopicTokenList:\n",
        "    tokenlist = []\n",
        "    eqlist = eqs.split(\" + \")\n",
        "    for idx, eq in enumerate(eqlist):\n",
        "      word = eq.split(\"*\")[1]\n",
        "      tokenlist.append(word.split('\"')[1])\n",
        "    TopicDic.append(tokenlist)\n",
        "\n",
        "  return TopicDic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU9E1zaJQQNi"
      },
      "outputs": [],
      "source": [
        "TopicWordList = parse_topic_token(RawTopicTokenList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGOkPCew-Ir"
      },
      "source": [
        "### Your work (2)\n",
        "\n",
        "- The number of topics should be adjusted by you, but once you give it around 10.\n",
        "- If there are many overlapping topics, you have to adjust it by decreasing the number of topics or by increasing the number of iterations (if you want, additional preprocessing is allowed).\n",
        "\n",
        "- Alternatively, there are some methods to decide the number of topics, such as perplexity and topic coherence. If you are interested in these topic model evaluation methods, please find the relevant resources.\n",
        "\n",
        "- You have to name each topic appropriately by interpreting the high-weighted words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwod5fJfxPXj"
      },
      "outputs": [],
      "source": [
        "################ Write Your code #####################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################################\n",
        "RawTopicTokenList = model.print_topics()\n",
        "TopicWordList = parse_topic_token(RawTopicTokenList)\n",
        "\n",
        "TopicName = [,,,,,] # enter the name of each topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTZOcct4oWR"
      },
      "source": [
        "## 3) Sentiment Analysis\n",
        "\n",
        "- The sentiment of the service features are estimated based on the sentiments of sentence containing service feature words.\n",
        "- We use VADER sentiment analysis to estimate sentiment intensity (score) of the each sentence\n",
        "- Score range of VADER : -1 (extremely negative) ~ +1 (extremely positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYJXnaPj4oWR",
        "outputId": "4a0964e1-0c80-4252-87db-ffaedb3f9a80"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnssoRtO4oWR",
        "outputId": "ef6b0faa-1989-4cad-ee43-7e805978e2cd"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "example1 = \"Location was great, nearby landmark\"\n",
        "example2 = \"Awesome view, Wow\"\n",
        "example3 = \"City veiw was not good\"\n",
        "example4 = \"Terrible room condition\"\n",
        "example5 = \"great quality internet\"\n",
        "\n",
        "print(\"Sentence  : [Sentiment score] \")\n",
        "print(\"{0} : {1}\".format(example1, analyser.polarity_scores(example1)['compound']))\n",
        "print(\"{0} : {1}\".format(example2, analyser.polarity_scores(example2)['compound']))\n",
        "print(\"{0} : {1}\".format(example3, analyser.polarity_scores(example3)['compound']))\n",
        "print(\"{0} : {1}\".format(example4, analyser.polarity_scores(example4)['compound']))\n",
        "print(\"{0} : {1}\".format(example5, analyser.polarity_scores(example5)['compound']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujHaQjBE4oWR"
      },
      "outputs": [],
      "source": [
        "#Sentiment analysis\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "def show_sentence_sentiment (TopicWordList_topic, TokenDataset, ReviewList):\n",
        "    for i,review in enumerate(TokenDataset):\n",
        "        SentenceTokenizedList = nltk.sent_tokenize(ReviewList[i])\n",
        "        for j,sent in enumerate(review):\n",
        "            for word in sent:\n",
        "                if word in TopicWordList_topic:\n",
        "                    print(\"word: \",word)\n",
        "                    print(\"tokenlist: \",sent)\n",
        "                    print(\"Sent: \",SentenceTokenizedList[j])\n",
        "                    print(\"Sentiment score: {}\\n\".format(analyser.polarity_scores(SentenceTokenizedList[j])['compound']))\n",
        "\n",
        "# Only for topic 1\n",
        "show_sentence_sentiment(TopicWordList[0], TokenDataset,  ReviewList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHh1I7qwshce"
      },
      "source": [
        "### Your Work (3)\n",
        "Modifying the above code (**'show_sentence_sentiment'** function), Create Review*SentimentLabel matrix (# of reviews * # of topics)\n",
        "- Review*SentimentLabel matrix\n",
        "    - shape : [review * service_feature(topic)]\n",
        "    - value : Sentiment label corresponding to sentiment intensity (score)\n",
        "    - Note : Convert sentiment intensity to sentiment label (see tutorial ppt file)\n",
        "\n",
        "- Output Variables name : ReviewSentimentMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfJoVAPXuYtn"
      },
      "outputs": [],
      "source": [
        "################ Write Your code #####################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdF0pl0N4oWR"
      },
      "source": [
        "## 4) Prediction Modeling on Customer Satisfaction\n",
        "\n",
        "- Input data (X, independence variables) : Review*sentiment_score matrix (Just use the data(ReviewSentimentMatrix) that you created in your work (3))\n",
        "   - shape : [review * service_feature(topic)]\n",
        "   - value : sentiment label\n",
        "- Output data (y, dependence variable) : star rating\n",
        "    - Using **StarRatingList** variable (already defined the above) and **preprocess** it!\n",
        "    - label  \n",
        "      - 0 : negative label (1,2,3 ratings)\n",
        "      - 1 : positive label (4,5 ratings)\n",
        "    - Output variable name : StarRatingLabels\n",
        "- Note\n",
        "    - We perform classification (categorical) task, not regression (continuous) task\n",
        "    - You have to remove the reviews that sentiment scores of all features are zero value. (These reviews are meaningless for analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7TbHoZV4oWR",
        "outputId": "cd79f4f2-f9a4-4544-9ca9-94734dbf8ebf"
      },
      "outputs": [],
      "source": [
        "#example\n",
        "#load the example input and output data. (This code and data are just for showing the example.)\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "'''Loading input, output for ML'''\n",
        "InputData_X = pickle.load(open(\"Example_Input_Data\", \"rb\"))\n",
        "OutputData_Y = pickle.load(open(\"Example_Output_Data\", \"rb\"))\n",
        "\n",
        "\n",
        "print(\"Shape of input data :{}\".format(InputData_X.shape))\n",
        "print(\"Shape of ouput data :{}\".format(OutputData_Y.shape))\n",
        "\n",
        "print(\"Input data : \\n{}\".format(InputData_X))\n",
        "print(\"Ouput data : {}\".format(OutputData_Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PvnETl84oWR",
        "outputId": "9ee151ce-57f2-4418-cfd9-d1765cf0ca2c"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# call the model\n",
        "logit=LogisticRegression()\n",
        "\n",
        "# train the model\n",
        "logit=logit.fit(InputData_X,OutputData_Y)\n",
        "\n",
        "print(\"Training Accuracy : {}\\n\".format(round(logit.score(InputData_X,OutputData_Y),3)))\n",
        "\n",
        "#print(\"coefficient : {}\".format(logit.coef_))\n",
        "\n",
        "TopicNameList = [\"Location\", \"View\", \"Breakfast\", \"Sleep Quality\", \"Bathroom\", \"Service\", \"Check\", \"Value\", \"Internet\"]\n",
        "print(\"Topic  (idx) : Coefficient\")\n",
        "for idx, coef_topic in enumerate(logit.coef_[0]):\n",
        "    print(\"{} ({}) : {}\".format(TopicNameList[idx],idx+1,round(coef_topic,3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbGZwBzv4oWS"
      },
      "outputs": [],
      "source": [
        "# performance : AverageSentimentScores\n",
        "Performance = InputData_X.mean(axis=0)\n",
        "MeanPerformance = Performance.mean()\n",
        "\n",
        "#importance\n",
        "Importance = logit.coef_[0]\n",
        "MeanImportance = Importance.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "XR8Dzm_74oWS",
        "outputId": "6cfb6f2f-e72c-441c-dba4-f524dd4dbd89"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(Performance, Importance, color = 'g')\n",
        "for idx, topic_name in enumerate(TopicNameList):\n",
        "  plt.text(Performance[idx]+0.005, Importance[idx]+0.005, topic_name)\n",
        "\n",
        "plt.axhline(MeanImportance, color = 'b', linestyle = '--')\n",
        "plt.axvline(MeanPerformance, color = 'b', linestyle = '--')\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.xlabel(\"Performance\")\n",
        "plt.title(\"Importance-Performance Analysis (IPA)\", fontsize = 20)\n",
        "plt.text(0.50, 0.26, \"Q2\", fontweight= \"bold\", fontsize = 15)\n",
        "plt.text(2.20, 0.26, \"Q1\", fontweight= \"bold\", fontsize = 15)\n",
        "plt.text(0.50, -0.10, \"Q3\", fontweight= \"bold\", fontsize = 15)\n",
        "plt.text(2.20, -0.10, \"Q4\", fontweight= \"bold\", fontsize = 15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx5iQPEivBSZ"
      },
      "source": [
        "### Your work (4)\n",
        "Modifying the above code, train the model using your own review*sentiment score matrix and conduct Importance-performance analysis (IPA)\n",
        "- If you want, you can preprocess the dataset to get high accuracy. \n",
        "- And, you can use other machine learning models, but it is necessary to interpret the result and conduct IPA through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiapWMkAcgrn"
      },
      "outputs": [],
      "source": [
        "InputData_X = ReviewSentimentMatrix\n",
        "OutputData_Y = StarRatingLabels\n",
        "\n",
        "################ Write Your code #####################\n",
        "\n",
        "\n",
        "#######################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff-0sxSZceDY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
